This blogpost is based on my first paper published earlier this year: "Valid prediction intervals for regression problems".<br><br>

Most people these days recognize that the standard approach to predictive modelling is not sufficient anymore.
The majority of papers and competitions (e.g. Kaggle) focus purely on predictive accuracy. This has two major downsides.
The first, overfitting, is a by now well-recognized problem, but the second, the lack of an uncertainty estimate, is much less studied.
A highly accurate model might still be horribly wrong in a small number of situations and in practice, think of self-driving cars or medical support systems,
it are often these critical cases that are important. Having no idea about how uncertain the model is, makes it hard to apply it in critical settings.<br><br>

When restricting to regression problems, the most straightforward approach to uncertainty quantification (UQ) is the construction of <b>prediction intervals</b> (PI).
To formalize this notion, denote the input space by \(\mathcal{X}\). A prediction interval for an instance \(x\in\mathcal{X}\) consists of an interval \([l(x),u(x)]\) such that
$$\mathbb{P}\Big(y\in[l(x),u(x)]\Big)\geq1-\alpha$$ for some predetermined <b>significance level</b> \(\alpha\in[0,1]\).
This condition states the <b>validity</b> of the intervals, if the intervals are required to satisfy this condition, we have a handle on how often they are correct.<br><br>

<div style="padding:0 5% 0 5%">
  For people familiar with parametric statistics, this validity condition might feel familiar.
  When estimating parameters, models often return confidence intervals (CI) which satisfy virtually the same condition.
  However, the main difference between CI and PI is that former will converge to a single point when enough data is gathered (and the correct model is used),
  while the latter will always be an interval since the data generating process is inherently stochastic.
</div>

<br><br>UNDER CONSTRUCTION

<br><br>Date: 18/11/2022
