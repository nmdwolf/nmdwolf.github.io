This blogpost is based on my first paper published earlier this year: "Valid prediction intervals for regression problems" in collaboration
with my promotors Willem Waegeman and Bernard De Baets.<br><br>

Most people these days recognize that the standard approach to predictive modelling is not sufficient anymore.
The majority of papers and competitions (e.g. Kaggle) focus purely on predictive accuracy. This has two major downsides.
The first, overfitting, is a by now well-recognized problem, but the second, the lack of an uncertainty estimate, is much less studied.
A highly accurate model might still be horribly wrong in a small number of situations and in practice, think of self-driving cars or medical support systems,
it are often these critical cases that are important. Having no idea about how uncertain the model is, makes it hard to apply it in critical settings.<br><br>

When restricting to regression problems, the most straightforward approach to uncertainty quantification (UQ) is the construction of <b>prediction intervals</b> (PI).
To formalize this notion, denote the input space by \(\mathcal{X}\). A prediction interval for an instance \(x\in\mathcal{X}\) consists of an interval \([l(x),u(x)]\subseteq\mathbb{R}\) such that
$$\mathbb{P}\Big(y\in[l(x),u(x)]\Big)\geq1-\alpha$$ for some predetermined <b>significance level</b> \(\alpha\in[0,1]\).
This condition states the <b>validity</b> of the intervals, if the intervals are required to satisfy this condition, we have a handle on how often they are correct.<br><br>

<div style="padding:0 5% 0 5%">
  For people familiar with parametric statistics, this validity condition might feel familiar.
  When estimating parameters, models often return confidence intervals (CI) which satisfy virtually the same condition.
  However, the main difference between CI and PI is that former will converge to a single point when enough data is gathered (and the correct model is used),
  while the latter will always be an interval since the data generating process is inherently stochastic.
</div>

The main problem, as is usually the case with inferential models, is that, in general, statistical conditions are hard to satisfy.
It is not hard to come up with interval estimators, but it is hard to come up with ones that are valid. Even those that are derived from
probabilistic assumptions often fail to be valid in practice. (This effectively turns them useless.) In general this problem can be ascribed to the following issues:
<ul>
  <li>Model misspecification.</li>
  <li>Lack of data.</li>
</ul>
The second reason is the most obvious one. Even if the parametric form of the data generating process is known, a lack of data will lead to wrong estimates
and the resulting PIs will not be valid. The first reason is harder to overcome. A straightforward solution would be to use nonparametric models,
but these models are harder to train since we cannot leverage any properties.

Some common approaches are <a href="https://en.wikipedia.org/wiki/Quantile_regression" target="_blank">quantile regression</a>,
<a href="https://en.wikipedia.org/wiki/Ensemble_learning" target="_blank">ensemble learning</a> and <a href="https://en.wikipedia.org/wiki/Bayesian_inference" target="_blank">Bayesian modelling</a>.
Bayesian modelling is often the most powerful approach, but without correctly specified priors, the resulting PIs will not be valid. Both quantile regression and ensemble modelling are nonparametric by nature, but suffer from other problems.
Quantile regression admits a theoretical derivation, but can only be used for predetermined quantiles, while ensemble models are versatile and robust, but still require a distributional prior
to obtain PIs (unless they are interpreted as a mixture model, but this still gives not theoretical guarantees).<br><br>

In comes <b>Conformal Prediction</b>. This framework, initially developed by V. Vovk, allows to turn any model into an interval estimator and, moreover,
this estimator will be valid. (In a next blog I might say a little more about the general framework of conformal prediction and how it also ecompasses classification problems etc.)
The main idea is to start from a <b>nonconformity score</b>$$A:\mathcal{X}\times\mathbb{R}\rightarrow\mathbb{R}$$, often extracted from a training set,
that tells us how different the point is from the training set (it is a measure of the nonconformity). Given a validation set \(\mathcal{V}\subset\mathcal{X}\times\mathbb{R}\), the inductive or split approach then proceeds as follows:
<ol>
  <li>Calculate the validation scores \(A(\mathcal{V})\).</li>
  <li>Determine the \(\left(1-\frac{1}{|\mathcal{V}|}\right)(1-\alpha)\)-quantile \(q^*\)$ of \(A(\mathcal{V})\).</li>
  <li>Construct the prediction set as \(\{y\in\mathbb{R}\mid A(x,y)\leq q^*\}\).</li>
</ol>
Note that for general scores \(A\)$, the resulting set might not be connected, i.e. it might not be an interval. (This is a general feature of conformal prediction.)
However, for the most commonly used scores, the sets will be intervals.<br><br>

The simplest example is the residual score $$A_\text{res}(x,y) := |y-\hat{\mu}(x)|$$ for a point-predictor \(\hat{\mu}:\mathcal{X}\rightarrow\mathbb{R}\). Since this function is essentially \(\mathcal{X}\)-independent,
it only depends on behaviour in the target space, the PIs will be homoskedastic (of equal length). This also allows to calculate the PIs essentialy with \(O(1)\) complexity: for any input \(x\in\mathcal{X}\)
the resulting PI can be constructed as $$\mathrm{PI}(x) := [\hat{\mu}(x)-q^*,\hat{\mu}(x)+q^*]\,,$$ as can easily be shown.
The homoskedasticity makes it a computationally efficient tool, but is it a realistic feature? The whole point was that certain data points
might be much harder to predict and, hence, have a larger uncertainty. Luckily, two other natural choices exist.<br><br>

The first one is a minor modification of the residual score: $$A_\sigma(x,y):=\frac{|y-\hat{\mu}(x)|}{\sigma(x)}\,,$$
where \(\sigma:\mathcal{X}\rightarrow\mathbb{R}\) is a "difficulty function", telling us how difficult the point is to estimate. The most common choice is the standard deviation.

<br><br>UNDER CONSTRUCTION

<br><br>Date: 18/11/2022<br>
